###12_ex1_start_md
[Insert examples]
###12_ex1_end

###12_ex2_start_py
date1 = datetime(2021, 10, 19, 10, 13)
date2 = datetime(2022, 5, 31, 9, 30)
date3 = datetime(2021, 11, 25, 15, 30)
###12_ex2_end

###12_ex3_start_py
print("hour", date1.hour)
print("year", date1.year)
print("minute", date2.minute)
print("month", date2.month)
###12_ex3_end

###12_ex4_start_py
for date in [date1, date2, date3]:
    print(date)
    print(date-timedelta(days=30))
    print()
###12_ex4_end

###12_ex5_start_py
print(pd.to_datetime("19 October 2021, 10:13 AM", format='%d %B %Y, %I:%M %p'))
print(pd.to_datetime("Oct 19 2021 - 10h13", format='%b %d %Y - %Hh%M'))
print(pd.to_datetime("2021-10-19 10:13:29", format='%Y-%m-%d %I:%M:%S'))
###12_ex5_end

###12_ex6_start_py
def extract_year(datetime):
    return datetime.year

display(covid.date.apply(extract_year))

# for a simple function that just extracts a single part of the time, can do this more easily:
#covid.date.dt.year
###12_ex6_end

###12_ex7_start_py
June1st = pd.to_datetime('2021-06-01', format='%Y-%m-%d')
covid[covid.date > June1st]['new_cases'].sum()
###12_ex7_end

###12_ex8_start_py
covid[
    (covid.index >  pd.to_datetime('2020', format='%Y')) & 
    (covid.index <  pd.to_datetime('2022', format='%Y'))].plot()
plt.show()
###12_ex8_end

###12_ex9_start_py
def clean_ufos(ufo):
    ufo.Time = pd.to_datetime(ufo.Time, format='%m/%d/%Y %H:%M')
    ufo.set_index('Time', inplace=True)
    
    return ufo
###12_ex9_end

###12_ex10_start_py
covid = covid.resample('W').sum()
covid.plot()
plt.show()
###12_ex10_end

###12_ex11_start_py
def tidy_eu_passengers(data):
    import pycountry
    import pandas as pd
    from re import match

    # ------------
    # TIDY COLUMNS
    # ------------

    # rename columns
    airlines = data.rename({
        "geo\\time":"country",
        "tra_meas":"measurement",
        "tra_cov": "coverage"
    }, axis="columns").copy()

    non_date_cols = list(airlines.columns[0:5])

    # remove the space in column names
    airlines.columns = airlines.columns.str.replace(' ', '')

    # just get the month columns
    filtered_values = list(filter(lambda v: match('\d+M\d+', v), airlines.columns))

    # reduce columns down to years with months
    airlines = airlines[non_date_cols+filtered_values]

    # make a date column
    airlines = pd.melt(airlines,
                       id_vars=non_date_cols,
                       var_name="date",
                       value_name='vals') 

    # ---------
    # TIDY DATA
    # ---------

    # replace the 'M' with a dash
    airlines.date = airlines.date.str.replace('M', '-')

    # change to a datetime
    airlines.date = pd.to_datetime(airlines.date, format='%Y-%m')

    #set the date as the index
    airlines.set_index('date', inplace=True)

    # get a dictionary with the codes and the country name
    country_dict = {}
    for country in airlines["country"].unique():
        try:
            country_dict[country] = pycountry.countries.lookup(country).name
        except:
            pass

    # use the dictionary to replace the codes
    airlines.country = airlines.country.replace(country_dict)
    # change ":" to nan
    airlines = airlines.replace(": ", np.nan)

    # change the values to float
    airlines.vals = airlines.vals.astype("float", errors='ignore')

    # sort earliest to most recent
    airlines.sort_index(inplace=True)

    return airlines
###12_ex11_end

###12_ex12_start_md
No, if you keep increasing the order the trend line would be a better fit, but would be worse off when predicting out of sample data (bias-variance tradeoff).
###12_ex12_end

###12_ex13_start_py
df = covid[mask].copy()

df.reset_index(inplace=True)

# save the dates
dates_ = df.date

# regplot don't play nice with datetimes so lets convert them to
# ordinal values
df["date"] = df.date.map(pd.Timestamp.toordinal)

fig, ax = plt.subplots()

sns.regplot(x='date', y='new_cases', data=df, ax=ax, scatter=False, order=2)
sns.lineplot(x='date', y='new_cases', data=df, ax=ax)

# set our labels back to the dates
ax.set_xticklabels(dates_.dt.date)

# legible labels
fig.autofmt_xdate(rotation=45, ha='right')

fig.tight_layout()
###12_ex13_end

###12_ex14_start_py
uk_flight_pass.plot();
###12_ex14_switch_md
It looks like there is some missing data between 2000 and 2003 so it may be worth removing the data before 2003. Also the effects of COVID means the data in 2020 stops following the initial trend. Due to the current frequency of the data, smoothing out spikes will not be helpful in this case. The datavisually doesn't appear to be fully homoskedastic as the difference between the number of flights in the later years looks larger than earlier years - this is something that may influence the accuracy of our models later on. If we wanted to reduce this heteroskedasticity we could take the logarithm of the number of passengers.
###12_ex14_end

###12_ex15_start_py
covid_forward = covid.shift(7)
covid_forward.index = covid_forward.index.shift(-7)
display(covid_forward.head(20))

fig, ax = plt.subplots()
covid_forward.plot(ax=ax);
covid_forward.shift(7).plot(ax=ax, figsize=(15,6));
###12_ex15_end

###12_ex16_start_py
uk_flight_pass_change = pd.DataFrame(uk_flight_pass).copy()

uk_flight_pass_change["change"] = uk_flight_pass_change - uk_flight_pass_change.shift(1)
uk_flight_pass_change.head()
###12_ex16_end

###12_ex17_start_py
fig, ax = plt.subplots(figsize=(15,6))
plot_acf(uk_flight_pass.diff().dropna(), lags=36, ax=ax);
###12_ex17_switch_md
It makes the peaks more prominant and also highlights how there is a peak positive correlation at 12 months and peak negative correlation at 6 months.
###12_ex17_end

###12_ex18_start_py
covid_ = covid.resample('W').sum()
covid_.plot();
plot_acf(covid_, lags=52);
###12_ex18_switch_md
It seems like a month ahead is still correlated but after that is unreliable. There doesn't appear to be any seasonality in this data. However as the data is non-stationary these results are probably not too reliable.
###12_ex18_end

###12_ex19_start_py
from statsmodels.tsa.seasonal import seasonal_decompose

flights_decomp = seasonal_decompose(uk_flight_pass, period=12) # older versions of statsmodels have freq instead of period
fig = flights_decomp.plot();
fig.set_size_inches((15,10))
# Tight layout to realign things
fig.tight_layout()
plt.show()

resid = flights_decomp.resid
resid.plot(figsize=(15,6))
plt.show()
fig, ax = plt.subplots(figsize=(15,6))
plot_acf(resid.dropna(), lags=12, ax = ax)
plt.show()

covid_decomp = seasonal_decompose(covid, period=12) # older versions of statsmodels have freq instead of period
fig = covid_decomp.plot();
fig.set_size_inches((15,10))
# Tight layout to realign things
fig.tight_layout()
plt.show()
###12_ex19_switch_md
TThe airline decomposition have residuals with seasonality (although not in the middle) and heteroskedasticity, so they aren't just random noise. This suggests the decomposition could be better. Beware! This method assumes the data are seasonal (because you've told it that) applying it to non-seasonal data will seem to work, but give nonsense results like the COVID one.
###12_ex19_end


###12_ex20_start_py
rail_train = uk_rail_pass.loc[:test_start-pd.DateOffset(months=3)]
rail_test = uk_rail_pass.loc[test_start:]

# copy the test data
y_hat = rail_test.to_frame().copy()
# get the last training value and add this as a value for each
# test data
y_hat['naive'] = rail_train[len(rail_train)-1]
# get the average of the training data
y_hat['avg_forecast'] = rail_train.mean()

rail_train.plot(label="Training Data", figsize=(15,6))
rail_test.plot(label="Testing Data")
y_hat['naive'].plot(label='Naive Forecast')
y_hat['avg_forecast'].plot(label='Simple Average Forecast')

plt.legend(fontsize=12)
plt.show()

print('Naive Model RMSE:', round(np.sqrt(mean_squared_error(rail_test, y_hat['naive'])),2))
print('Average Model RMSE:', round(np.sqrt(mean_squared_error(rail_test, y_hat['avg_forecast'])),2))
###12_ex20_switch_md
The average model is really bad because it does not account for the increasing trend. The Naive model is better but will get worse over time.
###12_ex20_end

###12_ex21_start_py
# fit our models
ARModel_12 = ARIMA(flights_train, order=(12,0,0)).fit() # just using AR 
print(ARModel_12.summary())

# get predictions and residuals
predictions_12 = ARModel_12.predict(start=pred_start_date, end=pred_end_date)
residuals_12 = flights_test-predictions_12

plt.figure(figsize=(10,4))
plt.plot(residuals_12)
plt.title("Residuals from AR Model")
plt.ylabel("Error")
plt.axhline(0, color="r", linestyle="--", alpha=0.2)
plt.show()

plt.figure(figsize=(10,4))

plt.plot(flights_train, label='train')
plt.plot(flights_test, label='test')
plt.plot(predictions_12, label='predictions')

ARforecast_12 = ARModel_12.get_forecast(flights_test.shape[0]).summary_frame()
plt.fill_between(flights_test.index, ARforecast_12.mean_ci_lower, ARforecast_12.mean_ci_upper, color='red', alpha=0.1)

plt.title("Airlines over time", fontsize=20)

plt.legend(loc='best');

plt.show()

print("Root Mean Squared Error:" , np.sqrt(mean_squared_error(flights_test, predictions_12)))
###12_ex21_switch_md
Using the PCAF above it looks like lags 1-5, 7, 9, 10-13 contain useful information (as well as some beyond this) as they are outside the shaded region... However our expected seasonality (12 months) doesn't look really pronounced here, but this is because we still have a trend here that will be affecting our PACF. Although we are adding a lot more unnessisary complexity to our model (a lot of lags are not significantly contributing), it is a better model than before.
###12_ex21_end

###12_ex22_start_py
ARIMAModel = ARIMA(np.log(flights_train), order=(15,1,12)).fit()
print(ARIMAModel.summary())

ARIMA_predictions = ARIMAModel.predict(start=pred_start_date, end=pred_end_date)

plt.figure(figsize=(15,6))

plt.plot(flights_train, label='train')
plt.plot(flights_test, label='test')
plt.plot(np.exp(ARIMA_predictions), label='predictions') # undo the log

plt.title("Differencing and Log (order = (15,1,12))", fontsize=20)

plt.legend(loc='best');

plt.show()

rmse = np.sqrt(mean_squared_error(flights_test, np.exp(ARIMA_predictions)))  # using np.exp() to cancel the log transformation
print('RMSE:', rmse.round(2))
###12_ex22_end

###12_ex23_start_py
rail_train = uk_rail_pass.loc[:test_start-pd.DateOffset(months=3)]
rail_test = uk_rail_pass.loc[test_start:]

autoarima_model = pm.auto_arima(np.log(rail_train),
                                trace=True,  # prints the search for optimal parameters
                                seasonal=True,  # whether our dataset displays seasonality or not
                                m=12,  # number of observations per seasonal cycle (i.e. seasonality)
                                d = 1 # The order of first-differencing
                               )

last_date = rail_train.index[-1]  # getting the last date from our training dataset
forecast_period = pd.date_range(start=last_date,  # as of pandas 1.3.4, this is not inclusive. Will change with pandas 1.4.0!
                                periods=24,
                                freq="MS",
                                )
# turn to quaterly
forecast_period = list(forecast_period[::3])
# add final quater
forecast_period.append(forecast_period[-1] + pd.DateOffset(months=3))
# delete first date
del forecast_period[0]

# predicting the same number of points as our forecast_period defined above
autoarima_forecast, conf_int = autoarima_model.predict(n_periods=len(forecast_period), return_conf_int=True)

# add the index to the forecast in a pandas series
autoarima_forecast = pd.Series(autoarima_forecast, index = forecast_period)

plt.figure(figsize=(15,6))

plt.plot(rail_train, label='train')
plt.plot(rail_test, label='test')
plt.plot(np.exp(autoarima_forecast), label='predictions')
# plot the confidence intervals on the forecasts
plt.fill_between(forecast_period, 
                 np.exp(conf_int[:, 0]), 
                 np.exp(conf_int[:, 1]), 
                 color='k', 
                 alpha=0.1,
                 )

plt.title("Trains over time", fontsize=20)

plt.legend(loc='best');

plt.show()

rmse = np.sqrt(mean_squared_error(rail_test, np.exp(autoarima_forecast)))  # using np.exp() to cancel the log transformation
print('RMSE:', rmse.round(2))
###12_ex23_end