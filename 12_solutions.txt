###12_ex1_start_md
[Insert examples]
###12_ex1_end

###12_ex2_start_py
date1 = datetime(2021, 10, 19, 10, 13)
date2 = datetime(2022, 5, 31, 9, 30)
date3 = datetime(2021, 11, 25, 15, 30)
###12_ex2_end

###12_ex3_start_py
print("hour", date1.hour)
print("year", date1.year)
print("minute", date2.minute)
print("month", date2.month)
###12_ex3_end

###12_ex4_start_py
for date in [date1, date2, date3]:
    print(date)
    print(date-timedelta(days=30))
    print()
###12_ex4_end

###12_ex5_start_py
print(pd.to_datetime("19 October 2021, 10:13 AM", format='%d %B %Y, %I:%M %p'))
print(pd.to_datetime("Oct 19 2021 - 10h13", format='%b %d %Y - %Hh%M'))
print(pd.to_datetime("2021-10-19 10:13:29", format='%Y-%m-%d %I:%M:%S'))
###12_ex5_end

###12_ex6_start_py
def extract_year(datetime):
    return datetime.year

display(covid.date.apply(extract_year))

# for a simple function that just extracts a single part of the time, can do this more easily:
#covid.date.dt.year
###12_ex6_end

###12_ex7_start_py
June1st = pd.to_datetime('2021-06-01', format='%Y-%m-%d')
covid[covid.date > June1st]['new_cases'].sum()
###12_ex7_end

###12_ex8_start_py
covid[
    (covid.index >  pd.to_datetime('2020', format='%Y')) & 
    (covid.index <  pd.to_datetime('2022', format='%Y'))].plot()
plt.show()
###12_ex8_end

###12_ex9_start_py
def clean_ufos(ufo):
    ufo.Time = pd.to_datetime(ufo.Time, format='%m/%d/%Y %H:%M')
    ufo.set_index('Time', inplace=True)
    
    return ufo
###12_ex9_end

###12_ex10_start_py
covid = covid.resample('W').sum()
covid.plot()
plt.show()
###12_ex10_end

###12_ex11_start_py
def tidy_eu_passengers(data):
    import pycountry
    import pandas as pd
    from re import match

    # ------------
    # TIDY COLUMNS
    # ------------

    # rename columns
    airlines = data.rename({
        "geo\\time":"country",
        "tra_meas":"measurement",
        "tra_cov": "coverage"
    }, axis="columns").copy()

    non_date_cols = list(airlines.columns[0:5])

    # remove the space in column names
    airlines.columns = airlines.columns.str.replace(' ', '')

    # just get the month columns
    filtered_values = list(filter(lambda v: match('\d+M\d+', v), airlines.columns))

    # reduce columns down to years with months
    airlines = airlines[non_date_cols+filtered_values]

    # make a date column
    airlines = pd.melt(airlines,
                       id_vars=non_date_cols,
                       var_name="date",
                       value_name='vals') 

    # ---------
    # TIDY DATA
    # ---------

    # replace the 'M' with a dash
    airlines.date = airlines.date.str.replace('M', '-')

    # change to a datetime
    airlines.date = pd.to_datetime(airlines.date, format='%Y-%m')

    #set the date as the index
    airlines.set_index('date', inplace=True)

    # get a dictionary with the codes and the country name
    country_dict = {}
    for country in airlines["country"].unique():
        try:
            country_dict[country] = pycountry.countries.lookup(country).name
        except:
            pass

    # use the dictionary to replace the codes
    airlines.country = airlines.country.replace(country_dict)
    # change ":" to nan
    airlines = airlines.replace(": ", np.nan)

    # change the values to float
    airlines.vals = airlines.vals.astype("float", errors='ignore')

    # sort earliest to most recent
    airlines.sort_index(inplace=True)

    return airlines
###12_ex11_end

###12_ex12_start_md
No, if you keep increasing the order the trend line would be a better fit, but would be worse off when predicting out of sample data (bias-variance tradeoff).
###12_ex12_end

###12_ex13_start_py
df = covid[mask].copy()

df.reset_index(inplace=True)

# save the dates
dates_ = df.date

# regplot don't play nice with datetimes so lets convert them to
# ordinal values
df["date"] = df.date.map(pd.Timestamp.toordinal)

fig, ax = plt.subplots()

sns.regplot(x='date', y='new_cases', data=df, ax=ax, scatter=False, order=2)
sns.lineplot(x='date', y='new_cases', data=df, ax=ax)

# set our labels back to the dates
ax.set_xticklabels(dates_.dt.date)

# legible labels
fig.autofmt_xdate(rotation=45, ha='right')

fig.tight_layout()
###12_ex13_end

###12_ex14_start_py
uk_flight_pass.plot();
###12_ex14_switch_md
It looks like there is some missing data between 2000 and 2003 so it may be worth removing the data before 2003. Also the effects of COVID means the data in 2020 stops following the initial trend. Due to the current frequency of the data, smoothing out spikes will not be helpful in this case. The datavisually doesn't appear to be fully homoskedastic as the difference between the number of flights in the later years looks larger than earlier years - this is something that may influence the accuracy of our models later on. If we wanted to reduce this heteroskedasticity we could take the logarithm of the number of passengers.
###12_ex14_end

###12_ex15_start_py
covid_forward = covid.shift(7)
covid_forward.index = covid_forward.index.shift(-7)
display(covid_forward.head(20))

fig, ax = plt.subplots()
covid_forward.plot(ax=ax);
covid_forward.shift(7).plot(ax=ax, figsize=(15,6));
###12_ex15_end

###12_ex16_start_py
uk_flight_pass_change = pd.DataFrame(uk_flight_pass).copy()

uk_flight_pass_change["change"] = uk_flight_pass_change - uk_flight_pass_change.shift(1)
uk_flight_pass_change.head()
###12_ex16_end

###12_ex17_start_py
fig, ax = plt.subplots(figsize=(15,6))
plot_acf(uk_flight_pass.diff().dropna(), lags=36, ax=ax);
###12_ex17_switch_md
It makes the peaks more prominant and also highlights how there is a peak positive correlation at 12 months and peak negative correlation at 6 months.
###12_ex17_end

###12_ex18_start_py
covid_ = covid.resample('W').sum()
covid_.plot();
plot_acf(covid_, lags=52);
###12_ex18_switch_md
It seems like a month ahead is still correlated but after that is unreliable. There doesn't appear to be any seasonality in this data. However as the data is non-stationary these results are probably not too reliable.
###12_ex18_end

###12_ex19_start_py
from statsmodels.tsa.seasonal import seasonal_decompose

flights_decomp = seasonal_decompose(uk_flight_pass, period=12) # older versions of statsmodels have freq instead of period
fig = flights_decomp.plot();
fig.set_size_inches((15,10))
# Tight layout to realign things
fig.tight_layout()
plt.show()

resid = flights_decomp.resid
resid.plot(figsize=(15,6))
plt.show()
fig, ax = plt.subplots(figsize=(15,6))
plot_acf(resid.dropna(), lags=12, ax = ax)
plt.show()

covid_decomp = seasonal_decompose(covid, period=12) # older versions of statsmodels have freq instead of period
fig = covid_decomp.plot();
fig.set_size_inches((15,10))
# Tight layout to realign things
fig.tight_layout()
plt.show()
###12_ex19_switch_md
TThe airline decomposition have residuals with seasonality (although not in the middle) and heteroskedasticity, so they aren't just random noise. This suggests the decomposition could be better. Beware! This method assumes the data are seasonal (because you've told it that) applying it to non-seasonal data will seem to work, but give nonsense results like the COVID one.
###12_ex19_end


###12_ex20_start_py
rail_train = uk_rail_pass.loc[:test_start-pd.DateOffset(months=3)]
rail_test = uk_rail_pass.loc[test_start:]

# copy the test data
y_hat = rail_test.to_frame().copy()
# get the last training value and add this as a value for each
# test data
y_hat['naive'] = rail_train[len(rail_train)-1]
# get the average of the training data
y_hat['avg_forecast'] = rail_train.mean()

rail_train.plot(label="Training Data", figsize=(15,6))
rail_test.plot(label="Testing Data")
y_hat['naive'].plot(label='Naive Forecast')
y_hat['avg_forecast'].plot(label='Simple Average Forecast')

plt.legend(fontsize=12)
plt.show()

print('Naive Model RMSE:', round(np.sqrt(mean_squared_error(rail_test, y_hat['naive'])),2))
print('Average Model RMSE:', round(np.sqrt(mean_squared_error(rail_test, y_hat['avg_forecast'])),2))
###12_ex20_switch_md
The average model is really bad because it does not account for the increasing trend. The Naive model is better but will get worse over time.
###12_ex20_end

###12_ex21_start_py
# fit our models
ARModel_12 = ARIMA(flights_train, order=(12,0,0)).fit() # just using AR 
print(ARModel_12.summary())

# get predictions and residuals
predictions_12 = ARModel_12.predict(start=pred_start_date, end=pred_end_date)
residuals_12 = flights_test-predictions_12

plt.figure(figsize=(10,4))
plt.plot(residuals_12)
plt.title("Residuals from AR Model")
plt.ylabel("Error")
plt.axhline(0, color="r", linestyle="--", alpha=0.2)
plt.show()

plt.figure(figsize=(10,4))

plt.plot(flights_train, label='train')
plt.plot(flights_test, label='test')
plt.plot(predictions_12, label='predictions')

ARforecast_12 = ARModel_12.get_forecast(flights_test.shape[0]).summary_frame()
plt.fill_between(flights_test.index, ARforecast_12.mean_ci_lower, ARforecast_12.mean_ci_upper, color='red', alpha=0.1)

plt.title("Airlines over time", fontsize=20)

plt.legend(loc='best');

plt.show()

print("Root Mean Squared Error:" , np.sqrt(mean_squared_error(flights_test, predictions_12)))
###12_ex21_switch_md
Using the PCAF above it looks like lags 1-5, 7, 9, 10-13 contain useful information (as well as some beyond this) as they are outside the shaded region... However our expected seasonality (12 months) doesn't look really pronounced here, but this is because we still have a trend here that will be affecting our PACF. Although we are adding a lot more unnessisary complexity to our model (a lot of lags are not significantly contributing), it is a better model than before.
###12_ex21_end

###12_ex22_start_py
ARIMAModel = ARIMA(np.log(flights_train), order=(15,1,12)).fit()
print(ARIMAModel.summary())

ARIMA_predictions = ARIMAModel.predict(start=pred_start_date, end=pred_end_date)

plt.figure(figsize=(15,6))

plt.plot(flights_train, label='train')
plt.plot(flights_test, label='test')
plt.plot(np.exp(ARIMA_predictions), label='predictions') # undo the log

plt.title("Differencing and Log (order = (15,1,12))", fontsize=20)

plt.legend(loc='best');

plt.show()

rmse = np.sqrt(mean_squared_error(flights_test, np.exp(ARIMA_predictions)))  # using np.exp() to cancel the log transformation
print('RMSE:', rmse.round(2))
###12_ex22_end

###12_ex25_start_py
rail_train = uk_rail_pass.loc[:test_start-pd.DateOffset(months=3)]
rail_test = uk_rail_pass.loc[test_start:]

autoarima_model = pm.auto_arima(np.log(rail_train),
                                trace=True,  # prints the search for optimal parameters
                                seasonal=True,  # whether our dataset displays seasonality or not
                                m=12,  # number of observations per seasonal cycle (i.e. seasonality)
                                d = 1 # The order of first-differencing
                               )

last_date = rail_train.index[-1]  # getting the last date from our training dataset
forecast_period = pd.date_range(start=last_date,  # as of pandas 1.3.4, this is not inclusive. Will change with pandas 1.4.0!
                                periods=24,
                                freq="MS",
                                )
# turn to quaterly
forecast_period = list(forecast_period[::3])
# add final quater
forecast_period.append(forecast_period[-1] + pd.DateOffset(months=3))
# delete first date
del forecast_period[0]

# predicting the same number of points as our forecast_period defined above
autoarima_forecast, conf_int = autoarima_model.predict(n_periods=len(forecast_period), return_conf_int=True)

# add the index to the forecast in a pandas series
autoarima_forecast = pd.Series(autoarima_forecast, index = forecast_period)

plt.figure(figsize=(15,6))

plt.plot(rail_train, label='train')
plt.plot(rail_test, label='test')
plt.plot(np.exp(autoarima_forecast), label='predictions')
# plot the confidence intervals on the forecasts
plt.fill_between(forecast_period, 
                 np.exp(conf_int[:, 0]), 
                 np.exp(conf_int[:, 1]), 
                 color='k', 
                 alpha=0.1,
                 )

plt.title("Trains over time", fontsize=20)

plt.legend(loc='best');

plt.show()

rmse = np.sqrt(mean_squared_error(rail_test, np.exp(autoarima_forecast)))  # using np.exp() to cancel the log transformation
print('RMSE:', rmse.round(2))
###12_ex25_end

###12_ex97_start_py
SARIMAX_predictions_rolling = pd.Series(dtype ='float64')
for end_date in flights_test.index:
    train_data_ = np.log(uk_flight_pass)[:end_date - pd.DateOffset(months=1)]
    model_fit_ = SARIMAX(train_data_, order=order, seasonal_order=seasonal_order).fit()
    pred_ = model_fit_.predict(end_date)
    SARIMAX_predictions_rolling.loc[end_date] = pred_.loc[end_date]
    
SARIMAX_residuals_rolling = flights_test - np.exp(SARIMAX_predictions_rolling)

plt.figure(figsize=(15,6))
plt.plot(SARIMAX_residuals_rolling)
plt.title("Residuals Rolling from SARIMA Model")
plt.ylabel("Error")
plt.axhline(0, color="r", linestyle="--", alpha=0.2)
plt.show()

plt.figure(figsize=(15,6))
plt.plot(flights_train, label='train')
plt.plot(flights_test, label='test')
plt.plot(np.exp(SARIMAX_predictions_rolling), label='predictions')
plt.title("Airlines over time (Rolling)", fontsize=20)
plt.legend(loc='best');
plt.show()

rmse = np.sqrt(mean_squared_error(flights_test, np.exp(SARIMAX_predictions_rolling)))  # using np.exp() to cancel the log transformation
print('RMSE:', round(rmse,2))
###12_ex97_end

###12_ex98_start_py
predictions_rolling_12 = pd.Series(index = flights_test.index, dtype ='float64')

quaterly_dates = list(flights_test.index[::3])
quaterly_dates.append(quaterly_dates[-1]+pd.DateOffset(months=3))
del quaterly_dates[0]
for end_date in quaterly_dates:
    train_data_ = uk_flight_pass[:end_date - pd.DateOffset(months=4)]
    model_fit_ = ARIMA(train_data_, order=(12,0,0)).fit()
    pred_ = model_fit_.predict(end_date - pd.DateOffset(months=3), end_date- pd.DateOffset(months=1))
    predictions_rolling_12.loc[end_date - pd.DateOffset(months=3):end_date] = pred_.loc[end_date - pd.DateOffset(months=3):end_date]
residuals_rolling_12 = flights_test - predictions_rolling_12

plt.figure(figsize=(10,4))
plt.plot(residuals_rolling_12)
plt.title("Residuals from AR Model")
plt.ylabel("Error")
plt.axhline(0, color="r", linestyle="--", alpha=0.2)
plt.show()

plt.figure(figsize=(10,4))

plt.plot(flights_train, label='train')
plt.plot(flights_test, label='test')
plt.plot(predictions_rolling_12, label='predictions')

plt.title("Airlines over time", fontsize=20)

plt.legend(loc='best');

plt.show()

print("Root Mean Squared Error:" , np.sqrt(mean_squared_error(flights_test, predictions_rolling_12)))
###12_ex98_switch_md
We see that if we were using it to predict every 3 months 3 months into the future, that its performance would be slightly better than if we used the same model once to predict 24 months in the future.
###12_ex98_end

###12_ex99_start_py
import requests
from re import match
import pickle
from IPython.display import Image

def load_data(update = False, display_head = False):

    if update:
        url = "https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?file=data/avia_paoa.tsv.gz"
        filename = "./Data/"+url.split("/")[-1]
        with open(filename, "wb") as f:
            r = requests.get(url)
            f.write(r.content)
        df = pd.read_csv(filename, compression='gzip', delimiter="\t|,", engine='python')
        # save to csv
        df.to_csv("./Data/passengerOverviewAirport_"+str(date.today())+".csv", index=False)
  
    # get a list of the data files
    onlyfiles = [f for f in os.listdir("Data") if os.path.isfile(os.path.join("Data", f))]
    # only get the airlines data
    passAirportlist = list(filter(re.compile("passengerOverviewAirport_*").match, onlyfiles))
    # load the latest data
    pass_raw = pd.read_csv("./Data/"+passAirportlist[-1])

    if display_head:
        # display the first few rows
        display(pass_raw.head())
        
    return pass_raw

def load_airport_codes(update = False, display_head = False):
    if update:
        # TODO
        pass
    
    # read in the airport code information
    airport_codes = pd.read_csv("./Data/airport-codes.csv")
    # set the codes as the index
    airport_codes.set_index('ident', inplace=True)
    
    if display_head:
        display(airport_codes.head())
        
    return airport_codes

# there are lots of airports we can choose from present in the data
def find_names(data, data_codes):
    data_ = data.copy()
    
    # split the airport code into the iso_country and ident
    data_[['iso_country', 'ident']] = data_.rep_airp.str.split('_' ,expand=True)

    # get a dictionary with the codes and the airport name
    airport_dict = {}
    ident_list = data_["ident"].unique()
    ident_list.sort()
    
    for ident in ident_list:
        try:
            airport_dict[ident] = airport_codes.loc[ident]['name']
        except:
            pass
        
    return airport_dict


# For heathrow at least we should also remove any data that is prior to 2003 as its (mostly) missing.
# and as we dont have much data on 2020 we should remove that (its very different!)
def tidy_eu_planes_airport(data, airport_codes, limit_start="2003-01-01", limit_end="2019-12-01", plot=False):
    # ------------
    # TIDY COLUMNS
    # ------------

    # rename columns
    airlines = data.rename({
        "rep_airp":"airport",
        "tra_meas":"measurement",
        "tra_cov\\time": "coverage"
    }, axis="columns").copy()

    non_date_cols = list(airlines.columns[0:5])

    # remove the space in column names
    airlines.columns = airlines.columns.str.replace(' ', '')

    # just get the month columns
    filtered_values = list(filter(lambda v: match('\d+M\d+', v), airlines.columns))

    # reduce columns down to years with months
    airlines = airlines[non_date_cols+filtered_values]

    # make a date column
    airlines = pd.melt(airlines,
                       id_vars=non_date_cols,
                       var_name="date",
                       value_name='vals') 

    # ---------
    # TIDY DATA
    # ---------

    # replace the 'M' with a dash
    airlines.date = airlines.date.str.replace('M', '-')

    # change to a datetime
    airlines.date = pd.to_datetime(airlines.date, format='%Y-%m')

    #set the date as the index
    airlines.set_index('date', inplace=True)

    # change ":" to nan
    airlines = airlines.replace(": ", np.nan)

    # change the values to float
    airlines.vals = airlines.vals.astype("float", errors='ignore')

    # split the airport code into the iso_country and ident
    airlines[['iso_country', 'ident']] = airlines.airport.str.split('_' ,expand=True)

    # remove old airport code column
    airlines.drop("airport", axis="columns", inplace=True)

    # get a dictionary with the codes and the airport name
    airline_dict = {}
    for ident in airlines["ident"].unique():
        try:
            airline_dict[ident] = airport_codes.loc[ident]['name']
        except:
            pass

    # use the dictionary to replace the codes
    airlines["airport"] = airlines.ident.replace(airline_dict)

    # remove iso stuff as don't need it anymore
    airlines.drop(["iso_country", "ident"], axis="columns", inplace=True)

    # reorder columns so airport is first
    cols = airlines.columns.tolist()
    airlines = airlines[cols[-1:] + cols[:-1]]
    
    # reorder from earliest date to latest
    airlines = airlines.sort_index(ascending=True)
    
    # reduce the data down to total passengers
    total_passengers = airlines[(airlines["unit"] == "PAS") &            # passengers
                                (airlines["measurement"] == "PAS_BRD") & # passengers on board
                                (airlines["schedule"] == "TOT") &        # national and international
                                (airlines["coverage"] == "TOTAL")        # national and international
                               ]["vals"].copy()                              # just get the values
    # we need to remove NAs
    total_passengers = total_passengers.dropna()

    # we should also remove any data that is prior to 2003 as its (mostly) missing.
    # and as we dont have much data on 2020 we should remove that (its very different!)
    total_passengers = total_passengers.loc[limit_start:limit_end]
    
    if plot:
        # Initial plot of the data
        total_passengers.plot()
        plt.show()

    return total_passengers

def get_auto_model(train, airport_code, trace = False, update = False):
    
    modelPath = os.path.join(os.path.curdir,"Model", AIRPORT_CODE+"_arima.pkl")
    
    if update or not os.path.exists(modelPath):
        
        autoarima_model = pm.auto_arima(np.log(train),
                                        trace=trace,  # prints the search for optimal parameters
                                        seasonal=True,  # whether our dataset displays seasonality or not
                                        m=12,  # number of observations per seasonal cycle (i.e. seasonality)
                                        d = 1, # The order of first-differencing
                                        random_state = 42 # ensures replicable results
                                       )
        # Serialize with Pickle
        with open(modelPath, 'wb') as pkl:
            pickle.dump(autoarima_model, pkl)
    else:
        # Now read it back and make a prediction
        with open(modelPath, 'rb') as pkl:
            autoarima_model = pickle.load(pkl)
            
    return autoarima_model

def plot_forecasts(model, train, test, airport_name, update = False):
    
    imgPath = os.path.join(os.path.curdir, "Figures", AIRPORT_CODE+"_arima.png")
    
    if update or not os.path.exists(imgPath):
    
        fig, axes = plt.subplots(3, 1, figsize=(12, 8))

        # predicting the training data
        train_forecast, train_conf_int = model.predict_in_sample(return_conf_int=True)
        # predicting the test data
        test_forecast, test_conf_int = model.predict(n_periods=len(test), return_conf_int=True)

        # update the model with the test data
        model.update(np.log(test))
        forecast_period = pd.date_range(start=test.index[-1] + pd.DateOffset(months=1),
                                        periods=24,
                                        freq="MS")
        # predicting the same number of points as our forecast_period defined above
        future_forecast, future_conf_int = model.predict(n_periods=len(forecast_period),
                                                                   return_conf_int=True)

        # add the index to the forecast in a pandas series
        future_forecast = pd.Series(future_forecast, index = forecast_period)

        title_dict = {0: "Training Predictions",
                      1: "Test set Predictions",
                      2: "Future Forecast"
                     }

        for i, (forecast, conf_int) in enumerate([(train_forecast, train_conf_int), 
                                                (test_forecast, test_conf_int),
                                                (future_forecast, future_conf_int),
                                               ]):

            # train plot
            axes[i].plot(train, label='train')
            axes[i].plot(test, label='test')
            axes[i].plot(np.exp(forecast), label='predictions', linestyle="--")

            # plot the confidence intervals on the forecasts
            axes[i].fill_between(forecast.index, 
                             np.exp(conf_int[:, 0]), 
                             np.exp(conf_int[:, 1]), 
                             color='k', 
                             alpha=0.1,
                             )

            axes[i].set_title(title_dict[i], fontsize=10)

        plt.legend(loc='best')

        plt.suptitle(airport_name)

        train_rmse = np.sqrt(mean_squared_error(train, np.exp(train_forecast)))  # using np.exp() to cancel the log transformation
        test_rmse = np.sqrt(mean_squared_error(test, np.exp(test_forecast)))  # using np.exp() to cancel the log transformation
        metrics_str = 'train RMSE: '+ str(train_rmse.round(2)) + '; test RMSE: ' + str(test_rmse.round(2))
        axes[i].text(0.8,-0.5, metrics_str, size=12, ha="center", 
                     transform=axes[i].transAxes)

        fig.tight_layout()
        
        plt.savefig(imgPath)

        plt.show()
        
    else:
        display(Image(imgPath))

# choose airport code (see `airport_dict`)
AIRPORT_CODE = "EGLL" # heathrow (EGLL)

# Update the model (if exists)?
UPDATE_MODEL = False
YRS_TEST = 2 # number of years to use as our test set
PROGRESS = True # Output info on model training progress

# Update the data?
UPDATE_DATA = False
UPDATE_CODES = False

imgPath = os.path.join(os.path.curdir, "Figures", AIRPORT_CODE+"_arima.png")
if UPDATE_MODEL or not os.path.exists(imgPath):
    # load flight data
    pass_raw = load_data(update = UPDATE_DATA)
    # load the airport codes
    airport_codes = load_airport_codes(update = UPDATE_CODES)
    # get the codes and names of airports
    airport_dict = find_names(pass_raw, airport_codes)
    # filter to airport of interest
    passAirports = pass_raw[pass_raw.rep_airp.str.contains(AIRPORT_CODE)]
    # tidy the airport data
    total_passengers = tidy_eu_planes_airport(passAirports, airport_codes, plot=PROGRESS)
    # create our training/test sets
    test_start = total_passengers.index[-1]-pd.DateOffset(years=YRS_TEST)
    flights_train = total_passengers.loc[:test_start]
    flights_test = total_passengers.loc[test_start+pd.DateOffset(months=1):]
    # train our model or load previously trained
    autoarima_model = get_auto_model(flights_train, AIRPORT_CODE, trace = PROGRESS, update = UPDATE_MODEL)
    # plot the forecasts
    plot_forecasts(autoarima_model, flights_train, flights_test, airport_dict[AIRPORT_CODE], update = UPDATE_MODEL)
    
else:
    display(Image(imgPath))
###12_ex99_end